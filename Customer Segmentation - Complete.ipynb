{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpOFUpPFBOjA"
   },
   "source": [
    "# Customer Segmentation - Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLeBimF88eyN"
   },
   "source": [
    "This notebook aims to analyse the purchases made by approximately 4,000 customers from a UK based online E-retail during the period of one year (from 01/12/2010 to 09/12/2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyCwMcI_pk2D"
   },
   "source": [
    "**1. Loading and preparing the original data**\n",
    "\n",
    "**2. Recency, Frequency, and Monetary Value analysis**\n",
    "\n",
    "**3. Data pre-processing for clustering**\n",
    "\n",
    "**4. Customer Segmentation with K-means**\n",
    "\n",
    "**5. Finding growth opportunities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYzF5bPz6vVv"
   },
   "source": [
    "## 0. Preliminary Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdpkkydA6zBt"
   },
   "source": [
    "Before starting any analysis, we make sure that the basic and general-use libraries (numpy, pandas, etc.) are correctly imported.\n",
    "\n",
    "We import the libraries including the Matplotlib-based statistical data visualization package \"seaborn\" to make graphs from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FrG6F2q5xd3",
    "outputId": "9025a7b7-834f-4c83-cf12-774478e35ced"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mohamed.a.eshra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mohamed.a.eshra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# data upload\n",
    "import pandas as pd\n",
    "import io\n",
    "# data manipulation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import LinearSVC\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqDzqpDf6_mV"
   },
   "source": [
    "## 1. Loading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_txI7dxI7ApV"
   },
   "source": [
    "We load the data into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx\"\n",
    "sheet1 = pd.read_excel(xls, sheet_name='Year 2009-2010')\n",
    "sheet2 = pd.read_excel(xls, sheet_name='Year 2010-2011')\n",
    "\n",
    "# We append the two DataFrames\n",
    "online = sheet1.append(sheet2, ignore_index=True)\n",
    "\n",
    "# And we convert the data type of the InvoiceDate column to datetime\n",
    "online['InvoiceDate'] = pd.to_datetime(online['InvoiceDate'])\n",
    "\n",
    "# And we rename several columns\n",
    "online.columns = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sgg0sv37iDD"
   },
   "source": [
    "We confirm that the dataset is appearing correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "O0J2bYxK9LjE",
    "outputId": "c0aab8f0-054f-4cff-9003-28d58a5e2f58"
   },
   "outputs": [],
   "source": [
    "online.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZU5MhIl7hCw",
    "outputId": "0679f541-ace4-45b2-fe34-6c65192f4dfd"
   },
   "outputs": [],
   "source": [
    "online.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcE2PXV3_Pyr"
   },
   "source": [
    "And it's always a good practice to review data types.\n",
    "As you can see, most of the columns are strings marked as objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVyfNqby-LZ7"
   },
   "source": [
    "Next, we review the data types, the number of null values and their percentage with respect to the total number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "LXwe1ImL9cp_",
    "outputId": "8af5cc87-1f01-487b-8b32-66cb534a6fd9"
   },
   "outputs": [],
   "source": [
    "# We reveiew the data types and the number and percentage of null values\n",
    "tab_info= pd.DataFrame(online.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info= tab_info.append(pd.DataFrame(online.isnull().sum()).T.rename(index={0:'number of null values'}))\n",
    "tab_info= tab_info.append(pd.DataFrame(online.isnull().sum()/online.shape[0]*100).T.\n",
    "                         rename(index={0:'null values (%)'}))\n",
    "tab_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZWUwSjH-7aX"
   },
   "source": [
    "When looking at the number of null values in the dataframe, we note that almost 23% of the entries are not assigned to a particular customer. Since these entries are not useful in this case, we remove all null values from the dataframe and confirm that no null values are left in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "U4TkHLzq_LzL",
    "outputId": "62bb6403-a148-421b-8b28-091c3c942062",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "online.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\n",
    "print('Dataframe dimensions:', online.shape)\n",
    "\n",
    "# gives some infos on columns types and numer of null values\n",
    "tab_info=pd.DataFrame(online.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(online.isnull().sum()).T.rename(index={0:'number of null values'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(online.isnull().sum()/online.shape[0]*100).T.\n",
    "                         rename(index={0:'null values (%)'}))\n",
    "tab_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDzUk8qU_yTZ"
   },
   "source": [
    "Finally, we check for duplicate entries and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJ9G2i5E_09j",
    "outputId": "df769d34-a1b0-440a-cf02-ef62da7da9c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Duplicate entries removed: {}'.format(online.duplicated().sum()))\n",
    "online.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we convert the data type of the CustomerID column to \"object\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = online.astype({\"CustomerID\": int})\n",
    "online = online.astype({\"CustomerID\": object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "pYMs6VwDydBE",
    "outputId": "7655c59e-d56d-4029-a16c-daf1c5898268"
   },
   "outputs": [],
   "source": [
    "online.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otCTwKjGyZyb"
   },
   "source": [
    "And we see that after removing null values and duplicates we are left with almost 800K observations from the original dataframe to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1OIMKgby7OO",
    "outputId": "d7c2da31-a617-4cc2-8cb1-1222f0fb00e3"
   },
   "outputs": [],
   "source": [
    "len(online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online['InvoiceNo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "online['order_canceled'] = online['InvoiceNo'].astype(str).apply(lambda x:int('C' in x))\n",
    "display(online[:15])\n",
    "\n",
    "n1 = online['order_canceled'].sum()\n",
    "n2 = online.shape[0]\n",
    "print('Number of orders canceled: {}/{} ({:.2f}%) '.format(n1, n2, n1/n2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online['QuantityCanceled'] = 0\n",
    "\n",
    "entry_to_remove = [] ; doubtfull_entry = []\n",
    "\n",
    "for index, col in  online.iterrows():\n",
    "    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n",
    "    df_test = online[(online['CustomerID'] == col['CustomerID']) &\n",
    "                         (online['StockCode']  == col['StockCode']) & \n",
    "                         (online['InvoiceDate'] < col['InvoiceDate']) & \n",
    "                         (online['Quantity']   > 0)].copy()\n",
    "    #_________________________________\n",
    "    # Cancelation WITHOUT counterpart\n",
    "    if (df_test.shape[0] == 0): \n",
    "        doubtfull_entry.append(index)\n",
    "    #________________________________\n",
    "    # Cancelation WITH a counterpart\n",
    "    elif (df_test.shape[0] == 1): \n",
    "        index_order = df_test.index[0]\n",
    "        online.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n",
    "        entry_to_remove.append(index)        \n",
    "    #______________________________________________________________\n",
    "    # Various counterparts exist in orders: we delete the last one\n",
    "    elif (df_test.shape[0] > 1): \n",
    "        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n",
    "        for ind, val in df_test.iterrows():\n",
    "            if val['Quantity'] < -col['Quantity']: continue\n",
    "            online.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n",
    "            entry_to_remove.append(index) \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entries_to_remove: {}\".format(len(entry_to_remove)))\n",
    "print(\"doubtfull_entries: {}\".format(len(doubtfull_entry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.drop(entry_to_remove, axis = 0, inplace = True)\n",
    "online.drop(doubtfull_entry, axis = 0, inplace = True)\n",
    "remaining_entries = online[(online['Quantity'] < 0) & (online['StockCode'] != 'D')]\n",
    "print(\"number of cancelled entries that have not been deleted: {}\".format(remaining_entries.shape[0]))\n",
    "remaining_entries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step of data preparation we remove the _StockCode_, _order_canceled_ and _QuantityCanceled_ since we don't need them anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online.drop(['StockCode', 'order_canceled', 'QuantityCanceled'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recency, Frequency, and Monetary Value analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calculate RFM values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work on behavioral customer segmentation based on three metrics:\n",
    "\n",
    "- Recency(R)\n",
    "\n",
    "- Frequency(F)\n",
    "\n",
    "- MonetaryValue(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a TotalSum column in the dataset by multiplying Quantity and UnitPrice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TotalSum column in the dataset by multiplying Quantity and UnitPrice\n",
    "online['TotalSum']= online['Quantity'] * online['UnitPrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create create a hypothetical snapshot_day variable as if we're doing analysis recently, that we can use to calculate recency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date = max(online.InvoiceDate) + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we calculate RFM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Recency, Frequency and Monetary value for each customer \n",
    "datamart = online.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "# Rename the columns \n",
    "datamart.rename(columns={'InvoiceDate': 'Recency',\n",
    "                         'InvoiceNo': 'Frequency',\n",
    "                         'TotalSum': 'MonetaryValue'}, inplace=True)\n",
    "\n",
    "# Print top 5 rows\n",
    "print(datamart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully built a dataset with recency, frequency, and monetary values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building RFM segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Calculate 3 groups for recency and frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now group the customers into three separate groups based on Recency, and Frequency.\n",
    "\n",
    "We will use the result from this step in the next step, where we will group customers based on the MonetaryValue and finally calculate an RFM_Score.\n",
    "Once completed, we print the results to the screen to make sure we have successfully created the quartile columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for Recency and Frequency\n",
    "r_labels = range(3, 0, -1); f_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "r_groups = pd.qcut(datamart['Recency'], q=3, labels=r_labels)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "f_groups = pd.qcut(datamart['Frequency'], q=3, labels=f_labels)\n",
    "\n",
    "# Create new columns R and F \n",
    "datamart = datamart.assign(R=r_groups.values, F=f_groups.values)\n",
    "\n",
    "# Create labels for MonetaryValue\n",
    "m_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "m_groups = pd.qcut(datamart['MonetaryValue'], q=3, labels=m_labels)\n",
    "\n",
    "# Create new column M\n",
    "datamart = datamart.assign(M=m_groups.values)\n",
    "\n",
    "datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will finalize the RFM segmentation by assigning customers to three groups based on the MonetaryValue percentiles and then calculate an RFM_Score which is a sum of the R, F, and M values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM_Score\n",
    "datamart['RFM_Score'] = datamart[['R','F','M']].sum(axis=1)\n",
    "print(datamart['RFM_Score'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now built the critical components for RFM segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Creating custom segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will  create a custom segmentation based on RFM_Score values. You will create a function to build segmentation and then assign it to each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create segments named Top, Middle, Low. If the RFM score is greater than or equal to 9, the level should be \"Top\". If it's between 6 and 9 it should be \"Middle\", and otherwise it should be \"Low\".\n",
    "\n",
    "We will then apply the rfm_level function and store it as RFM_Level value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rfm_level function\n",
    "def rfm_level(df):\n",
    "    if df['RFM_Score'] >= 9:\n",
    "        return 'Top'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['RFM_Score'] < 9)):\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Create a new variable RFM_Level\n",
    "datamart['RFM_Level'] = datamart.apply(rfm_level, axis=1)\n",
    "\n",
    "# Print the header with top 5 rows to the console\n",
    "print(datamart.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully created a custom segment based on RFM Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Analyzing custom segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we will analyze average values of Recency, Frequency and MonetaryValue for the custom segments we've created.\n",
    "\n",
    "We calculate the averages for Recency, Frequency and MonetaryValue for each _RFM_Level_ segment.\n",
    "\n",
    "As the last column, we return the size of each segment passing count to the MonetaryValue column next to the mean.\n",
    "We then print the aggregated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values for each RFM_Level, and return a size of each segment \n",
    "rfm_level_agg = datamart.groupby('RFM_Level').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "  \n",
    "  # Return the size of each segment\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building RFM segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created segments, we want to make predictions. However, we first need to master practical data preparation methods to ensure our k-means clustering algorithm will uncover well-separated, sensible segments.\n",
    "\n",
    "We will load the dataset with RFM values that we calculated previously as _datamart_rfm_ to explore the distributions of the RFM values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with RFM values that we calculated previously as datamart_rfm\n",
    "datamart_rfm = datamart[['Recency', 'Frequency', 'MonetaryValue']]\n",
    "datamart_rfm = datamart_rfm[datamart_rfm['MonetaryValue'] > 0]\n",
    "datamart_rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the distribution of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 10), dpi=80)\n",
    "\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.histplot(datamart_rfm['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.histplot(datamart_rfm['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.histplot(datamart_rfm['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a good understanding about the distributions of RFM variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre-process RFM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the variables are skewed and are on different scales, we will now un-skew and normalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply log transformation to unskew the _datamart_rfm_ and store it as datamart_log. Then, we initialize a StandardScaler() instance as scaler and fit it on the datamart_log data. \n",
    "\n",
    "Next, we transform the data by scaling and centering it with scaler.\n",
    "\n",
    "Then, we create a pandas DataFrame from _'datamart_normalized'_ by adding index and column names from datamart_rfm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_rfm.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unskew the data\n",
    "datamart_log = np.log(datamart_rfm)\n",
    "\n",
    "# Initialize a standard scaler and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datamart_log)\n",
    "\n",
    "# Scale and center the data\n",
    "datamart_normalized = scaler.transform(datamart_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "datamart_normalized_df = pd.DataFrame(data=datamart_normalized, index=datamart_rfm.index, columns=datamart_rfm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have unskewed and normalized the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualize the normalized variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the normalized and unskewed variables to see the difference in the distribution as well as the range of the values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the distribution of the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 10), dpi=80)\n",
    "\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.histplot(datamart_normalized_df['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.histplot(datamart_normalized_df['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.histplot(datamart_normalized_df['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the skewness is managed after applying these transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Correlation of the normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = datamart_normalized_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Customer Segmentation with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running k-means and assigning labels to raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the data that we pre-processed in the previous section to identify customer clusters based on their recency, frequency, and monetary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datamart_normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- More recent customers (low recency) tend to buy more frequently.\n",
    "- Monetary value tend to be higher for more recent customers (low recency).\n",
    "- Customers that buys more frequently tend to generate a higher monetary value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Choosing the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the sum of squared errors for different number of clusters ranging from 1 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KMeans and calculate SSE for each *k*\n",
    "sse = {}\n",
    "\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, 11):\n",
    "  \n",
    "    # Initialize KMeans with k clusters and fit it on the normalized dataset\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(datamart_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of dictionary\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the sum of squared errors for each value of k and identify if there is an elbow. This will guide us towards the recommended number of clusters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the plot title \"The Elbow Method\"\n",
    "plt.title('The Elbow Method')\n",
    "\n",
    "# Add X-axis label \"k\"\n",
    "plt.xlabel('k')\n",
    "\n",
    "# Add Y-axis label \"SSE\"\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the elbow is clearly around 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building customer segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a 3 clusters with k-means clustering. We have loaded the pre-processed RFM dataset as _datamart_normalized_df_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_normalized= datamart_normalized_df.values\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_rfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now built a 3-cluster segmentation and extracted cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now analyse the average RFM values of the three clusters that we have created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame by adding a new cluster label column\n",
    "datamart_rfm_k3 = datamart_rfm.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group the data by cluster\n",
    "grouped = datamart_rfm_k3.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFM values and segment sizes per cluster value\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart_rfm_k3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the differences in RFM values of these segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualizing and evaluating the customer segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the customer segments, with the yellow cluster represting the \"high\" value customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "high = datamart_rfm_k3.query('Cluster == 0')\n",
    "mid = datamart_rfm_k3.query('Cluster == 2')\n",
    "low = datamart_rfm_k3.query('Cluster == 1')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "g1= (low['Frequency'].values, low['Recency'].values, low['MonetaryValue'].values)\n",
    "g2 = (mid['Frequency'].values, mid['Recency'].values, mid['MonetaryValue'].values)\n",
    "g3= (high['Frequency'].values, high['Recency'].values, high['MonetaryValue'].values)\n",
    "\n",
    "data = [g1, g2, g3]\n",
    "colors = ['#3ACF1B', '#FF7F00', '#3382FF']\n",
    "groups = ['Low', 'Med', 'High']\n",
    "\n",
    "for data, color, group in zip(data, colors, groups):\n",
    "    x, y, z = data\n",
    "    ax.scatter(x, y, z, alpha=0.5, c=color, label=group)\n",
    "\n",
    "# Make legend\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_ylabel('Recency')\n",
    "    ax.set_zlabel('MonetaryValue')\n",
    "    ax.set_title('Spatial Representation of Segments', loc='left')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhoutte Score\n",
    "score = silhouette_score(datamart_normalized, kmeans.labels_, metric='euclidean').round(2)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Finding growth opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Top product keywords for low-value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get to know our customers, analyze the customer segments and understand what kind of customer groups and behaviors they represent.\n",
    "We will focus on the low-value customer segments (cluster number 1), which is the customer segment that has the highest recenecy, lowest frequency and monetary value to improve our marketing strategy and offer them products that are related to their shopping interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(online, datamart_rfm_k3, on='CustomerID')\n",
    "merged_df = merged_df[['CustomerID', 'Country', 'Description', 'Cluster']].drop_duplicates()\n",
    "merged_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "low_value = merged_df[merged_df['Cluster'] == 1]\n",
    "low_value.reset_index(drop=True, inplace = True)\n",
    "low_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we extract from the Description variable the information that will prove useful. To do this, we use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "def keywords_inventory(dataframe, column = 'Description'):\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "    keywords_roots  = dict()  # collect the words / root\n",
    "    keywords_select = dict()  # association: root <-> keyword\n",
    "    category_keys   = []\n",
    "    count_keywords  = dict()\n",
    "    icount = 0\n",
    "    for s in dataframe[column]:\n",
    "        if pd.isnull(s): continue\n",
    "        lines = s.lower()\n",
    "        tokenized = nltk.word_tokenize(lines)\n",
    "        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "        \n",
    "        for t in nouns:\n",
    "            t = t.lower() ; root = stemmer.stem(t)\n",
    "            if root in keywords_roots:                \n",
    "                keywords_roots[root].add(t)\n",
    "                count_keywords[root] += 1                \n",
    "            else:\n",
    "                keywords_roots[root] = {t}\n",
    "                count_keywords[root] = 1\n",
    "    \n",
    "    for s in keywords_roots.keys():\n",
    "        if len(keywords_roots[s]) > 1:  \n",
    "            min_length = 1000\n",
    "            for k in keywords_roots[s]:\n",
    "                if len(k) < min_length:\n",
    "                    clef = k ; min_length = len(k)            \n",
    "            category_keys.append(clef)\n",
    "            keywords_select[s] = clef\n",
    "        else:\n",
    "            category_keys.append(list(keywords_roots[s])[0])\n",
    "            keywords_select[s] = list(keywords_roots[s])[0]\n",
    "                   \n",
    "    print(\"Number of keywords in variable '{}': {}\".format(column,len(category_keys)))\n",
    "    return category_keys, keywords_roots, keywords_select, count_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as input the dataframe and analyzes the content of the Description column by performing the following operations:\n",
    "\n",
    "*   Extract the names (proper, common) appearing in the products description.\n",
    "*   We extract the root of the word and aggregate the set of names associated with this particular root for each name.\n",
    "*  Count the number of times each root appears in the dataframe.\n",
    "*  When several words are listed for the same root, we consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants).\n",
    "\n",
    "The first step of the analysis is to retrieve the list of products from the \"Description\" variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.DataFrame(low_value['Description'].unique()).rename(columns = {0:'Description'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this list is created, we use the function that we previously defined in order to analyze the description of the various products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of this function returns three variables:\n",
    "\n",
    "*   Keywords: the list of extracted keywords\n",
    "*   Keywords_roots: a dictionary where the keys are the keywords' roots and the values are the lists of words associated with those roots\n",
    "*   Count_keywords: dictionary listing the number of times every word is used\n",
    "\n",
    "At this point, we convert the count_keywords dictionary into a list, to sort the keywords according to their occurences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_products = []\n",
    "for k,v in count_keywords.items():\n",
    "    list_products.append([keywords_select[k],v])\n",
    "list_products.sort(key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using list_products, we create a representation of the most common keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "list_words = sorted(list_products, key = lambda x:x[1], reverse = True)\n",
    "#_______________________________\n",
    "plt.rc('font', weight='normal')\n",
    "fig, ax = plt.subplots(figsize=(7, 25))\n",
    "y_axis = [i[1] for i in list_words[:125]]\n",
    "x_axis = [k for k,i in enumerate(list_words[:125])]\n",
    "x_label = [i[0] for i in list_words[:125]]\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.yticks(x_axis, x_label)\n",
    "plt.xlabel(\"Number of occurences\", fontsize = 18, labelpad = 10)\n",
    "ax.barh(x_axis, y_axis, align = 'center')\n",
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "#_______________________________________________________________________________________\n",
    "plt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='w',fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the \"Words occurance graph\" that low-value customers' top 3 products contained the words \"heart, \"set\", and \"pink\".\n",
    "This information may be helpfull for improving marketing efforts directed to those customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Where are the low-value customers located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = low_value[['CustomerID', 'Description', 'Country']].groupby(['CustomerID', 'Description', 'Country']).count()\n",
    "temp = temp.reset_index(drop = False)\n",
    "countries = temp['Country'].value_counts()\n",
    "print('Number of countries in the dataframe: {}'.format(len(countries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_temp= low_value[['CustomerID', 'Country']]\n",
    "cust_grouped = cust_temp.groupby('CustomerID')\n",
    "\n",
    "cust_country = cust_grouped.apply(lambda x: x['Country'].unique())\n",
    "\n",
    "cust_country_df = pd.DataFrame({'CustomerID':cust_country.index, 'Country':cust_country.values})\n",
    "\n",
    "# Remove square brackets from the \"Country\" column\n",
    "cust_country_df['Country'] = cust_country_df['Country'].str[0]\n",
    "\n",
    "cust_country_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "cust_country_df['Country'].value_counts(normalize=True).plot.bar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that almost 100% of low-value customers are in the United Kingdom."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting customer purchases of an online retail using Machine Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
